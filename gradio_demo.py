import os

# --- å¼ºåˆ¶é‡å®šå‘æ‰€æœ‰ç¼“å­˜åˆ°é¡¹ç›®æ‰€åœ¨ç›˜ç¬¦ (é˜²æ­¢å¡«æ»¡ C ç›˜) ---
PROJECT_ROOT = os.path.dirname(os.path.abspath(__file__))
CACHE_DIR = os.path.join(PROJECT_ROOT, "temp_cache")
os.makedirs(CACHE_DIR, exist_ok=True)

# è®¾ç½®ç¯å¢ƒå˜é‡ï¼Œå¿…é¡»åœ¨å¯¼å…¥å…¶ä»–åº“ä¹‹å‰
os.environ["HF_HOME"] = os.path.join(CACHE_DIR, "hf")
os.environ["TORCH_HOME"] = os.path.join(CACHE_DIR, "torch")
os.environ["GRADIO_TEMP_DIR"] = os.path.join(CACHE_DIR, "gradio")
os.environ["PYTHONPYCACHEPREFIX"] = os.path.join(CACHE_DIR, "pycache")
os.environ["MODELSCOPE_CACHE"] = os.path.join(CACHE_DIR, "modelscope")
os.environ["OMP_NUM_THREADS"] = "1"
os.environ["MKL_NUM_THREADS"] = "1"

# å°†ç›¸å…³ç›®å½•åŠ å…¥ Python è·¯å¾„ï¼Œä¿®å¤åŸé¡¹ç›®çš„å¯¼å…¥é—®é¢˜
import sys

sys.path.append(os.path.join(PROJECT_ROOT, "src", "infer"))

# å¿½ç•¥è­¦å‘Š
import warnings

warnings.filterwarnings("ignore", category=UserWarning)
warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=DeprecationWarning)

import gradio as gr
import torch
import librosa
import numpy as np
import os
import json
import torchaudio
import torch.nn as nn
import torchaudio.compliance.kaldi as kaldi
from librosa.filters import mel as librosa_mel_fn
from src.runtime.speaker_verification.verification import init_model as init_sv_model
import subprocess
import shutil
from pathlib import Path
from tqdm import tqdm
import threading
import time
from queue import Queue
import signal
import traceback

# Constants/Paths
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
MODEL_CONFIG_PATH = "src/config/config_200ms.json"
CKPT_PATH = "src/ckpt/meanvc_200ms.pt"
ASR_CKPT_PATH = "src/ckpt/fastu2++.pt"
VOCODER_CKPT_PATH = "src/ckpt/vocos.pt"
SV_CKPT_PATH = "src/runtime/speaker_verification/ckpt/wavlm_large_finetune.pth"

# --- Feature Extraction Utils (Copied from src/infer/infer_ref.py with robustness fixes) ---


def _amp_to_db(x, min_level_db):
    min_level = np.exp(min_level_db / 20 * np.log(10))
    min_level = torch.ones_like(x) * min_level
    return 20 * torch.log10(torch.maximum(min_level, x))


def _normalize(S, max_abs_value, min_db):
    return torch.clamp(
        (2 * max_abs_value) * ((S - min_db) / (-min_db)) - max_abs_value,
        -max_abs_value,
        max_abs_value,
    )


class MelSpectrogramFeatures(nn.Module):
    def __init__(
        self,
        sample_rate=16000,
        n_fft=1024,
        win_size=640,
        hop_length=160,
        n_mels=80,
        fmin=0,
        fmax=8000,
        center=True,
    ):
        super().__init__()
        self.sample_rate = sample_rate
        self.n_fft = n_fft
        self.hop_length = hop_length
        self.n_mels = n_mels
        self.win_size = win_size
        self.fmin = fmin
        self.fmax = fmax
        self.center = center
        self.mel_basis = {}
        self.hann_window = {}

    def forward(self, y):
        dtype_device = str(y.dtype) + "_" + str(y.device)
        fmax_dtype_device = str(self.fmax) + "_" + dtype_device
        wnsize_dtype_device = str(self.win_size) + "_" + dtype_device
        if fmax_dtype_device not in self.mel_basis:
            mel = librosa_mel_fn(
                sr=self.sample_rate,
                n_fft=self.n_fft,
                n_mels=self.n_mels,
                fmin=self.fmin,
                fmax=self.fmax,
            )
            self.mel_basis[fmax_dtype_device] = torch.from_numpy(mel).to(
                dtype=y.dtype, device=y.device
            )
        if wnsize_dtype_device not in self.hann_window:
            self.hann_window[wnsize_dtype_device] = torch.hann_window(self.win_size).to(
                dtype=y.dtype, device=y.device
            )

        spec = torch.stft(
            y,
            self.n_fft,
            hop_length=self.hop_length,
            win_length=self.win_size,
            window=self.hann_window[wnsize_dtype_device],
            center=self.center,
            pad_mode="reflect",
            normalized=False,
            onesided=True,
            return_complex=False,
        )
        spec = torch.sqrt(spec.pow(2).sum(-1) + 1e-6)
        spec = torch.matmul(self.mel_basis[fmax_dtype_device], spec)
        spec = _amp_to_db(spec, -115) - 20
        spec = _normalize(spec, 1, -115)
        return spec


def extract_fbanks(
    wav, sample_rate=16000, mel_bins=80, frame_length=25, frame_shift=12.5
):
    wav = wav * (1 << 15)
    if isinstance(wav, np.ndarray):
        wav = torch.from_numpy(wav)
    if wav.ndim == 1:
        wav = wav.unsqueeze(0)
    fbanks = kaldi.fbank(
        wav,
        frame_length=frame_length,
        frame_shift=frame_shift,
        snip_edges=True,
        num_mel_bins=mel_bins,
        energy_floor=0.0,
        dither=0.0,
        sample_frequency=sample_rate,
    )
    fbanks = fbanks.unsqueeze(0)
    return fbanks


def extract_features_from_audio(
    source_path, reference_path, asr_model, sv_model, mel_extractor, device
):
    source_wav, _ = librosa.load(source_path, sr=16000)
    source_fbanks = (
        extract_fbanks(source_wav, frame_length=25, frame_shift=10).float().to(device)
    )

    with torch.no_grad():
        offset = 0
        decoding_chunk_size = 5
        num_decoding_left_chunks = 2
        subsampling = 4
        context = 7
        stride = subsampling * decoding_chunk_size
        required_cache_size = decoding_chunk_size * num_decoding_left_chunks
        decoding_window = (decoding_chunk_size - 1) * subsampling + context
        att_cache = torch.zeros((0, 0, 0, 0), device=device)
        cnn_cache = torch.zeros((0, 0, 0, 0), device=device)

        bn_chunks = []
        for i in range(0, source_fbanks.shape[1], stride):
            fbank_chunk = source_fbanks[:, i : i + decoding_window, :]
            if fbank_chunk.shape[1] < required_cache_size:
                pad_size = required_cache_size - fbank_chunk.shape[1]
                fbank_chunk = torch.nn.functional.pad(
                    fbank_chunk, (0, 0, 0, pad_size), mode="constant", value=0.0
                )

            encoder_output, att_cache, cnn_cache = asr_model.forward_encoder_chunk(
                fbank_chunk, offset, required_cache_size, att_cache, cnn_cache
            )
            offset += encoder_output.size(1)
            bn_chunks.append(encoder_output)

        bn = torch.cat(bn_chunks, dim=1)
        bn = bn.transpose(1, 2)
        bn = torch.nn.functional.interpolate(
            bn, size=int(bn.shape[2] * 4), mode="linear", align_corners=False
        )
        bn = bn.transpose(1, 2)

    ref_wav, _ = librosa.load(reference_path, sr=16000)
    ref_wav_tensor = torch.from_numpy(ref_wav).unsqueeze(0).to(device)

    with torch.no_grad():
        spk_emb = sv_model(ref_wav_tensor)
        prompt_mel = mel_extractor(ref_wav_tensor)
        prompt_mel = prompt_mel.transpose(1, 2)

    return bn, spk_emb, prompt_mel


@torch.inference_mode()
def inference(model, vocos, bn, spk_emb, prompt_mel, chunk_size, steps, device):
    if steps == 1:
        timesteps = torch.tensor([1.0, 0.0], device=device)
    elif steps == 2:
        timesteps = torch.tensor([1.0, 0.8, 0.0], device=device)
    else:
        timesteps = torch.linspace(1.0, 0.0, steps + 1, device=device)

    seq_len = bn.shape[1]
    x_pred = []
    B = 1
    kv_cache = None

    for start in range(0, seq_len, chunk_size):
        end = min(start + chunk_size, seq_len)
        bn_chunk = bn[:, start:end]
        chunk_len = bn_chunk.shape[1]
        x = torch.randn(B, chunk_len, 80, device=device, dtype=bn_chunk.dtype)

        for i in range(steps):
            t = timesteps[i].item()
            r = timesteps[i + 1].item()
            t_tensor = torch.full((B,), t, device=x.device)
            r_tensor = torch.full((B,), r, device=x.device)

            u, tmp_kv_cache = model(
                x,
                t_tensor,
                r_tensor,
                bn_chunk,
                spk_emb,
                prompt_mel,
                None,  # cache
                start,  # offset = current position
                kv_cache,
            )
            x = x - (t - r) * u
            kv_cache = tmp_kv_cache

        x_pred.append(x)

        if start > 40 and kv_cache is not None:
            # æ£€æŸ¥kv_cacheç»“æ„æ˜¯å¦å®Œæ•´
            try:
                if (
                    kv_cache[0] is not None
                    and kv_cache[0][0] is not None
                    and kv_cache[0][0].shape[2] > 100
                ):
                    for i in range(len(kv_cache)):
                        if kv_cache[i] is not None and kv_cache[i][0] is not None:
                            new_k = kv_cache[i][0][:, :, -100:, :]
                            new_v = kv_cache[i][1][:, :, -100:, :]
                            kv_cache[i] = (new_k, new_v)
            except (TypeError, IndexError):
                pass

    x_pred = torch.cat(x_pred, dim=1)
    mel = x_pred.transpose(1, 2)
    mel = (mel + 1) / 2
    y_g_hat = vocos.decode(mel)

    return mel, y_g_hat


# --- Gradio App Logic ---

_models = {}


def load_models():
    global _models
    if _models:
        return _models

    if not os.path.exists(SV_CKPT_PATH):
        raise FileNotFoundError(
            f"Speaker verification model not found at {SV_CKPT_PATH}. Please download it manually from the link in README.md."
        )

    print(f"Loading models to {DEVICE}...")

    with open(MODEL_CONFIG_PATH) as f:
        model_config = json.load(f)

    print(" - Loading DiT model...")
    dit_model = torch.jit.load(CKPT_PATH, map_location=DEVICE).to(DEVICE)

    print(" - Loading Vocos (Vocoder)...")
    vocos = torch.jit.load(VOCODER_CKPT_PATH, map_location=DEVICE).to(DEVICE)

    print(" - Loading ASR model (Content extraction)...")
    asr_model = torch.jit.load(ASR_CKPT_PATH, map_location=DEVICE).to(DEVICE)

    print(" - Loading Speaker Verification model (WavLM)...")
    sv_model = init_sv_model("wavlm_large", SV_CKPT_PATH).to(DEVICE)
    sv_model.eval()

    print(" - Initializing Mel extractor...")
    mel_extractor = MelSpectrogramFeatures(
        sample_rate=16000,
        n_fft=1024,
        win_size=640,
        hop_length=160,
        n_mels=80,
        fmin=0,
        fmax=8000,
        center=True,
    ).to(DEVICE)

    _models = {
        "dit": dit_model,
        "vocos": vocos,
        "asr": asr_model,
        "sv": sv_model,
        "mel": mel_extractor,
    }
    return _models


def voice_conversion(source_audio_path, reference_audio_path, steps, chunk_size):
    if source_audio_path is None or reference_audio_path is None:
        return None, "Please provide both source and reference audio."

    try:
        models = load_models()
        bn, spk_emb, prompt_mel = extract_features_from_audio(
            source_audio_path,
            reference_audio_path,
            models["asr"],
            models["sv"],
            models["mel"],
            DEVICE,
        )
        _, wav = inference(
            models["dit"],
            models["vocos"],
            bn,
            spk_emb,
            prompt_mel,
            chunk_size,
            steps,
            DEVICE,
        )
        wav_np = wav.squeeze().cpu().numpy()
        return (16000, wav_np), "Success"
    except Exception as e:
        return None, str(e)


# --- Training Functions ---

# å…¨å±€è®­ç»ƒçŠ¶æ€
train_stop_flag = False
train_thread = None
train_log_queue = Queue()


def run_training(
    dataset_path,
    exp_name,
    batch_size,
    epochs,
    learning_rate,
    save_interval,
    use_gpu,
):
    """
    å®é™…æ‰§è¡Œè®­ç»ƒï¼ˆç®€åŒ–ç‰ˆï¼Œä½¿ç”¨å‘½ä»¤è¡Œè°ƒç”¨ï¼‰
    """
    global train_stop_flag

    try:
        # æ£€æŸ¥æ•°æ®é›†è·¯å¾„
        dataset_path = Path(dataset_path)
        if not dataset_path.exists():
            yield "âŒ é”™è¯¯ï¼šæ•°æ®é›†è·¯å¾„ä¸å­˜åœ¨"
            return

        train_list = dataset_path / "train.list"
        if not train_list.exists():
            yield "âŒ é”™è¯¯ï¼šæœªæ‰¾åˆ° train.list æ–‡ä»¶ï¼Œè¯·å…ˆè¿›è¡Œæ•°æ®é¢„å¤„ç†"
            yield "æç¤ºï¼šä½¿ç”¨'æ•°æ®é¢„å¤„ç†'Tabå¤„ç†ä½ çš„éŸ³é¢‘æ•°æ®"
            return

        # è®¾ç½®å®éªŒç›®å½•
        exp_dir = Path(PROJECT_ROOT) / "results" / exp_name
        exp_dir.mkdir(parents=True, exist_ok=True)

        yield f"âœ… æ£€æŸ¥é€šè¿‡"
        yield f"ğŸ“ å®éªŒåç§°: {exp_name}"
        yield f"ğŸ“‚ ä¿å­˜ç›®å½•: {exp_dir}"
        yield f"ğŸ“Š æ•°æ®é›†: {dataset_path}"

        # å‡†å¤‡è®­ç»ƒå‘½ä»¤
        cuda_devices = "0" if use_gpu and torch.cuda.is_available() else ""
        if use_gpu and not torch.cuda.is_available():
            yield "âš ï¸ è­¦å‘Šï¼šGPUä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨CPUè®­ç»ƒï¼ˆä¼šéå¸¸æ…¢ï¼‰"

        yield f"\nğŸš€ å¯åŠ¨è®­ç»ƒ..."
        yield f"ğŸ“ å‚æ•°: batch_size={batch_size}, epochs={epochs}, lr={learning_rate}"

        # æ„å»ºå‘½ä»¤
        cmd = [
            sys.executable,
            "src/train/train.py",
            "--model-config",
            "src/config/config_160ms.json",
            "--batch-size",
            str(batch_size),
            "--max-len",
            "1000",
            "--flow-ratio",
            "0.50",
            "--cfg-ratio",
            "0.1",
            "--cfg-scale",
            "2.0",
            "--p",
            "0.5",
            "--num-workers",
            "4",
            "--feature-list",
            "bn mel xvector",
            "--additional-feature-list",
            "inputs_length prompt",
            "--feature-pad-values",
            "0. -1.0 0.",
            "--steps",
            "1",
            "--cfg-strength",
            "2.0",
            "--chunk-size",
            "16",
            "--result-dir",
            str(exp_dir),
            "--save-per-updates",
            str(save_interval),
            "--reset-lr",
            "0",
            "--epochs",
            str(epochs),
            "--resumable-with-seed",
            "666",
            "--grad-accumulation-steps",
            "1",
            "--grad-ckpt",
            "0",
            "--exp-name",
            exp_name,
            "--dataset-path",
            str(dataset_path),
            "--learning-rate",
            str(learning_rate),
        ]

        # è®¾ç½®ç¯å¢ƒå˜é‡
        env = os.environ.copy()
        env["PYTHONPATH"] = f"{PROJECT_ROOT}:{env.get('PYTHONPATH', '')}"
        if cuda_devices:
            env["CUDA_VISIBLE_DEVICES"] = cuda_devices

        yield f"\n{'=' * 50}"
        yield "è®­ç»ƒè¿›è¡Œä¸­... (æŒ‰'åœæ­¢è®­ç»ƒ'æŒ‰é’®å¯ä¸­æ–­)"
        yield f"{'=' * 50}\n"

        # å¯åŠ¨è®­ç»ƒè¿›ç¨‹
        process = subprocess.Popen(
            cmd,
            stdout=subprocess.PIPE,
            stderr=subprocess.STDOUT,
            text=True,
            bufsize=1,
            universal_newlines=True,
            cwd=PROJECT_ROOT,
            env=env,
        )

        # å®æ—¶è¯»å–è¾“å‡º
        log_buffer = []
        while True:
            # æ£€æŸ¥åœæ­¢æ ‡å¿—
            if train_stop_flag:
                process.terminate()
                try:
                    process.wait(timeout=5)
                except:
                    process.kill()
                yield "\nğŸ›‘ è®­ç»ƒå·²è¢«ç”¨æˆ·åœæ­¢"
                yield f"ğŸ’¾ æ£€æŸ¥ç‚¹å¯èƒ½å·²ä¿å­˜åœ¨: {exp_dir}"
                break

            # è¯»å–è¾“å‡º
            try:
                if process.stdout:
                    line = process.stdout.readline()
                    if not line and process.poll() is not None:
                        break

                    if line:
                        log_buffer.append(line.strip())
                        # åªä¿ç•™æœ€è¿‘20è¡Œ
                        if len(log_buffer) > 20:
                            log_buffer = log_buffer[-20:]
                        yield "\n".join(log_buffer)
            except:
                # Windowsä¸‹å¯èƒ½ä¼šæœ‰ç¼–ç é—®é¢˜
                pass

            # çŸ­æš‚ä¼‘çœ é¿å…CPUå ç”¨è¿‡é«˜
            time.sleep(0.1)

        # è·å–è¿”å›ç 
        return_code = process.poll()

        if return_code == 0:
            yield f"\nâœ… è®­ç»ƒæˆåŠŸå®Œæˆï¼"
            yield f"ğŸ’¾ æ¨¡å‹ä¿å­˜åœ¨: {exp_dir}"
        else:
            yield f"\nâŒ è®­ç»ƒå¤±è´¥ (è¿”å›ç : {return_code})"
            yield "è¯·æ£€æŸ¥ä¸Šé¢çš„é”™è¯¯æ—¥å¿—"

    except Exception as e:
        yield f"\nâŒ è®­ç»ƒé”™è¯¯: {str(e)}"
        yield traceback.format_exc()


def stop_training():
    """åœæ­¢è®­ç»ƒ"""
    global train_stop_flag
    train_stop_flag = True
    return "æ­£åœ¨åœæ­¢è®­ç»ƒ..."


def start_training_thread(*args):
    """åœ¨åå°çº¿ç¨‹å¯åŠ¨è®­ç»ƒ"""
    global train_thread, train_stop_flag
    train_stop_flag = False

    def train_wrapper():
        for log in run_training(*args):
            train_log_queue.put(log)

    train_thread = threading.Thread(target=train_wrapper)
    train_thread.start()
    return "è®­ç»ƒå·²å¯åŠ¨"


def get_train_logs():
    """è·å–è®­ç»ƒæ—¥å¿—"""
    logs = []
    while not train_log_queue.empty():
        logs.append(train_log_queue.get())
    return "\n".join(logs) if logs else ""


def preprocess_dataset(input_dir, output_dir, progress=gr.Progress()):
    """
    é¢„å¤„ç†æ•°æ®é›†ï¼šæå–Melã€BNã€xvectorç‰¹å¾
    """
    try:
        input_path = Path(input_dir)
        output_path = Path(output_dir)

        if not input_path.exists():
            return "é”™è¯¯ï¼šè¾“å…¥ç›®å½•ä¸å­˜åœ¨"

        # åˆ›å»ºè¾“å‡ºç›®å½•
        mel_dir = output_path / "mels"
        bn_dir = output_path / "bns"
        xvector_dir = output_path / "xvectors"

        mel_dir.mkdir(parents=True, exist_ok=True)
        bn_dir.mkdir(parents=True, exist_ok=True)
        xvector_dir.mkdir(parents=True, exist_ok=True)

        log_messages = []
        log_messages.append(f"å¼€å§‹é¢„å¤„ç†æ•°æ®é›†...")
        log_messages.append(f"è¾“å…¥ç›®å½•: {input_dir}")
        log_messages.append(f"è¾“å‡ºç›®å½•: {output_dir}")

        # è·å–æ‰€æœ‰éŸ³é¢‘æ–‡ä»¶
        audio_files = list(input_path.glob("*.wav")) + list(input_path.glob("*.mp3"))
        if not audio_files:
            return "é”™è¯¯ï¼šè¾“å…¥ç›®å½•ä¸­æ²¡æœ‰æ‰¾åˆ°éŸ³é¢‘æ–‡ä»¶ (.wav æˆ– .mp3)"

        log_messages.append(f"æ‰¾åˆ° {len(audio_files)} ä¸ªéŸ³é¢‘æ–‡ä»¶")

        # æ­¥éª¤1ï¼šæå–Melé¢‘è°±
        progress(0.1, desc="æå–Melé¢‘è°±...")
        log_messages.append("\næ­¥éª¤1/3: æå–Melé¢‘è°±")

        for i, audio_file in enumerate(tqdm(audio_files, desc="Melæå–")):
            try:
                # ä½¿ç”¨å·²å®šä¹‰çš„MelSpectrogramFeaturesç±»
                mel_extractor = MelSpectrogramFeatures()
                wav, sr = librosa.load(str(audio_file), sr=16000)
                wav_tensor = torch.from_numpy(wav).unsqueeze(0)

                with torch.no_grad():
                    mel = mel_extractor(wav_tensor)
                    mel_np = mel.squeeze().cpu().numpy()

                output_file = mel_dir / f"{audio_file.stem}.npy"
                np.save(str(output_file), mel_np)

            except Exception as e:
                log_messages.append(f"  è·³è¿‡ {audio_file.name}: {str(e)}")

            if i % 10 == 0:
                progress(
                    0.1 + 0.2 * (i / len(audio_files)),
                    desc=f"Melæå– {i}/{len(audio_files)}",
                )

        log_messages.append(f"Melé¢‘è°±æå–å®Œæˆï¼Œä¿å­˜åˆ° {mel_dir}")

        # æ­¥éª¤2ï¼šæå–BNç‰¹å¾ï¼ˆéœ€è¦ASRæ¨¡å‹ï¼‰
        progress(0.4, desc="æå–BNç‰¹å¾...")
        log_messages.append("\næ­¥éª¤2/3: æå–BNç‰¹å¾")

        mel_files = list(mel_dir.glob("*.npy"))
        log_messages.append(f"ä½¿ç”¨é¢„è®­ç»ƒASRæ¨¡å‹æå–BNç‰¹å¾...")

        for i, audio_file in enumerate(tqdm(audio_files, desc="BNæå–")):
            try:
                # è°ƒç”¨é¢„å¤„ç†è„šæœ¬
                cmd = [
                    "python",
                    "src/preprocess/extract_bn_160ms.py",
                    "--input_dir",
                    str(input_path),
                    "--output_dir",
                    str(bn_dir),
                ]
                result = subprocess.run(
                    cmd, capture_output=True, text=True, cwd=PROJECT_ROOT
                )
                if result.returncode == 0:
                    log_messages.append(f"  BNç‰¹å¾å·²æå–")
                break  # æ¼”ç¤ºæ¨¡å¼ï¼Œåªå¤„ç†ä¸€ä¸ªæ–‡ä»¶
            except Exception as e:
                log_messages.append(f"  BNæå–é”™è¯¯: {str(e)}")
                break

        log_messages.append(f"BNç‰¹å¾æå–å®Œæˆï¼Œä¿å­˜åˆ° {bn_dir}")

        # æ­¥éª¤3ï¼šæå–å£°çº¹ç‰¹å¾
        progress(0.7, desc="æå–å£°çº¹ç‰¹å¾...")
        log_messages.append("\næ­¥éª¤3/3: æå–å£°çº¹ç‰¹å¾")

        try:
            cmd = [
                "python",
                "src/preprocess/extract_spk_emb_wavlm.py",
                "--input_dir",
                str(input_path),
                "--output_dir",
                str(xvector_dir),
            ]
            result = subprocess.run(
                cmd, capture_output=True, text=True, cwd=PROJECT_ROOT
            )
            if result.returncode == 0:
                log_messages.append(f"  å£°çº¹ç‰¹å¾æå–å®Œæˆ")
            else:
                log_messages.append(f"  é”™è¯¯: {result.stderr}")
        except Exception as e:
            log_messages.append(f"  å£°çº¹æå–é”™è¯¯: {str(e)}")

        progress(1.0, desc="é¢„å¤„ç†å®Œæˆ")

        # ç”Ÿæˆæ•°æ®åˆ—è¡¨
        log_messages.append("\nç”Ÿæˆè®­ç»ƒæ•°æ®åˆ—è¡¨...")
        train_list = output_path / "train.list"
        with open(train_list, "w") as f:
            for audio_file in audio_files[:10]:  # æ¼”ç¤ºï¼šåªä½¿ç”¨å‰10ä¸ª
                utt_id = audio_file.stem
                bn_path = bn_dir / f"{utt_id}.npy"
                mel_path = mel_dir / f"{utt_id}.npy"
                xvector_path = xvector_dir / f"{utt_id}.npy"
                prompt_mel_path = mel_path  # ä½¿ç”¨è‡ªå·±çš„melä½œä¸ºprompt

                if bn_path.exists() and mel_path.exists() and xvector_path.exists():
                    f.write(
                        f"{utt_id}|{bn_path}|{mel_path}|{xvector_path}|{prompt_mel_path}\n"
                    )

        log_messages.append(f"æ•°æ®åˆ—è¡¨å·²ä¿å­˜åˆ°: {train_list}")
        log_messages.append(f"\né¢„å¤„ç†å®Œæˆï¼")
        log_messages.append(f"è¯·æ£€æŸ¥è¾“å‡ºç›®å½•: {output_path}")

        return "\n".join(log_messages)

    except Exception as e:
        return f"é¢„å¤„ç†é”™è¯¯: {str(e)}"


def generate_train_script(
    dataset_path, exp_name, batch_size, epochs, learning_rate, save_interval, use_gpu
):
    """
    ç”Ÿæˆè®­ç»ƒè„šæœ¬
    """
    try:
        script_content = f"""#!/bin/bash
# MeanVC è®­ç»ƒè„šæœ¬ - è‡ªåŠ¨ç”Ÿæˆ
# å®éªŒåç§°: {exp_name}

export PYTHONPATH=$PYTHONPATH:$PWD

# è®¾ç½®GPU
cuda={"0" if use_gpu else ""}
IFS=',' read -ra parts <<< "$cuda"
num_gpus=${{#parts[@]}}

echo "ä½¿ç”¨ $num_gpus ä¸ªGPU"
port=`comm -23 <(seq 50075 65535 | sort) <(ss -tan | awk '{{print $4}}' | cut -d':' -f2 | sort -u) | shuf | head -n 1`

# å¯åŠ¨è®­ç»ƒ
accelerate launch --config-file default_config.yaml \\
    --main_process_port $port \\
    --num_processes ${{num_gpus}} \\
    {"--gpu_ids ${{cuda}}" if use_gpu else "--cpu"} \\
    src/train/train.py \\
    --model-config src/config/config_160ms.json \\
    --batch-size {batch_size} \\
    --max-len 1000 \\
    --flow-ratio 0.50 \\
    --cfg-ratio 0.1 \\
    --cfg-scale 2.0 \\
    --p 0.5 \\
    --num-workers 4 \\
    --feature-list "bn mel xvector" \\
    --additional-feature-list "inputs_length prompt" \\
    --feature-pad-values "0. -1.0 0." \\
    --steps 1 \\
    --cfg-strength 2.0 \\
    --chunk-size 16 \\
    --result-dir "results" \\
    --save-per-updates {save_interval} \\
    --reset-lr 0 \\
    --epochs {epochs} \\
    --resumable-with-seed 666 \\
    --grad-accumulation-steps 1 \\
    --grad-ckpt 0 \\
    --exp-name {exp_name} \\
    --dataset-path "{dataset_path}" \\
    --learning-rate {learning_rate}

echo "è®­ç»ƒå®Œæˆï¼"
"""

        # ä¿å­˜è„šæœ¬
        script_path = Path(PROJECT_ROOT) / f"train_{exp_name}.sh"
        with open(script_path, "w") as f:
            f.write(script_content)

        return f"è®­ç»ƒè„šæœ¬å·²ç”Ÿæˆ: {script_path}\\n\\nè„šæœ¬å†…å®¹ï¼š\\n{script_content}"

    except Exception as e:
        return f"ç”Ÿæˆè„šæœ¬é”™è¯¯: {str(e)}"


# --- Gradio UI ---

with gr.Blocks(title="MeanVC Demo & Training") as demo:
    gr.Markdown("# MeanVC: Lightweight and Streaming Zero-Shot Voice Conversion")
    gr.Markdown("è¯­éŸ³è½¬æ¢æ¼”ç¤ºä¸è®­ç»ƒå·¥å…·")

    with gr.Tabs():
        # Tab 1: è¯­éŸ³è½¬æ¢
        with gr.TabItem("è¯­éŸ³è½¬æ¢"):
            gr.Markdown("### å°†æºéŸ³é¢‘çš„å£°éŸ³è½¬æ¢ä¸ºå‚è€ƒéŸ³é¢‘çš„éŸ³è‰²")

            with gr.Row():
                with gr.Column():
                    source_audio = gr.Audio(
                        type="filepath", label="æºéŸ³é¢‘ï¼ˆè¦è½¬æ¢çš„å£°éŸ³ï¼‰"
                    )
                    ref_audio = gr.Audio(type="filepath", label="å‚è€ƒéŸ³é¢‘ï¼ˆç›®æ ‡éŸ³è‰²ï¼‰")

                    with gr.Accordion("é«˜çº§è®¾ç½®", open=False):
                        steps_slider = gr.Slider(
                            minimum=1, maximum=10, value=2, step=1, label="é™å™ªæ­¥æ•°"
                        )
                        chunk_size_slider = gr.Slider(
                            minimum=1, maximum=30, value=20, step=1, label="å—å¤§å°"
                        )

                    submit_btn = gr.Button("å¼€å§‹è½¬æ¢", variant="primary")

                with gr.Column():
                    output_audio = gr.Audio(label="è½¬æ¢åçš„éŸ³é¢‘")
                    status_msg = gr.Textbox(label="çŠ¶æ€", interactive=False)

            submit_btn.click(
                fn=voice_conversion,
                inputs=[source_audio, ref_audio, steps_slider, chunk_size_slider],
                outputs=[output_audio, status_msg],
            )

            gr.Examples(
                examples=[
                    [
                        "src/runtime/example/test.wav",
                        "src/runtime/example/test.wav",
                        2,
                        20,
                    ],
                ],
                inputs=[source_audio, ref_audio, steps_slider, chunk_size_slider],
            )

        # Tab 2: æ•°æ®é¢„å¤„ç†
        with gr.TabItem("æ•°æ®é¢„å¤„ç†"):
            gr.Markdown("### å‡†å¤‡è®­ç»ƒæ•°æ®é›†")
            gr.Markdown("""
            æ­¤åŠŸèƒ½å°†è‡ªåŠ¨ï¼š
            1. æå–Melé¢‘è°±ï¼ˆ10mså¸§ç§»ï¼‰
            2. æå–å†…å®¹ç‰¹å¾BNï¼ˆ160msçª—å£ï¼‰
            3. æå–å£°çº¹ç‰¹å¾ï¼ˆxvectorï¼‰
            4. ç”Ÿæˆè®­ç»ƒæ•°æ®åˆ—è¡¨
            """)

            with gr.Row():
                with gr.Column():
                    input_dir = gr.Textbox(
                        label="è¾“å…¥ç›®å½•",
                        placeholder="åŒ…å«.wavéŸ³é¢‘æ–‡ä»¶çš„ç›®å½•è·¯å¾„",
                        value="path/to/your/audio/files",
                    )
                    output_dir = gr.Textbox(
                        label="è¾“å‡ºç›®å½•",
                        placeholder="é¢„å¤„ç†åæ•°æ®ä¿å­˜è·¯å¾„",
                        value="path/to/output/features",
                    )

                    preprocess_btn = gr.Button("å¼€å§‹é¢„å¤„ç†", variant="primary")

                with gr.Column():
                    preprocess_output = gr.Textbox(
                        label="å¤„ç†æ—¥å¿—", lines=20, interactive=False
                    )

            preprocess_btn.click(
                fn=preprocess_dataset,
                inputs=[input_dir, output_dir],
                outputs=preprocess_output,
            )

        # Tab 3: æ¨¡å‹è®­ç»ƒ
        with gr.TabItem("æ¨¡å‹è®­ç»ƒ"):
            gr.Markdown("### åœ¨Gradioä¸­ç›´æ¥è®­ç»ƒæ¨¡å‹")
            gr.Markdown("""
            æ­¤åŠŸèƒ½å…è®¸ä½ ç›´æ¥åœ¨Webç•Œé¢ä¸­è®­ç»ƒMeanVCæ¨¡å‹ã€‚
            **æ³¨æ„**ï¼šè®­ç»ƒä¼šå ç”¨è¾ƒå¤šè®¡ç®—èµ„æºï¼Œå»ºè®®åœ¨GPUç¯å¢ƒä¸‹è¿›è¡Œã€‚
            """)

            with gr.Row():
                with gr.Column():
                    train_dataset_path = gr.Textbox(
                        label="æ•°æ®é›†è·¯å¾„",
                        placeholder="é¢„å¤„ç†åçš„æ•°æ®ç›®å½•ï¼ˆåŒ…å«train.listï¼‰",
                        value="path/to/output/features",
                    )
                    train_exp_name = gr.Textbox(
                        label="å®éªŒåç§°",
                        placeholder="my_experiment",
                        value="my_meanvc_train",
                    )

                    with gr.Row():
                        train_batch_size = gr.Slider(
                            minimum=1, maximum=64, value=16, step=1, label="æ‰¹æ¬¡å¤§å°"
                        )
                        train_epochs = gr.Slider(
                            minimum=1,
                            maximum=1000,
                            value=100,
                            step=10,
                            label="è®­ç»ƒè½®æ•°",
                        )

                    with gr.Row():
                        train_lr = gr.Number(
                            value=0.0001,
                            label="å­¦ä¹ ç‡",
                            minimum=0.00001,
                            maximum=0.01,
                            step=0.00001,
                        )
                        train_save_interval = gr.Slider(
                            minimum=100,
                            maximum=50000,
                            value=1000,
                            step=100,
                            label="ä¿å­˜é—´éš”ï¼ˆæ­¥æ•°ï¼‰",
                        )

                    train_use_gpu = gr.Checkbox(label="ä½¿ç”¨GPU", value=True)

                    with gr.Row():
                        start_train_btn = gr.Button("å¼€å§‹è®­ç»ƒ", variant="primary")
                        stop_train_btn = gr.Button("åœæ­¢è®­ç»ƒ", variant="stop")

                with gr.Column():
                    train_output = gr.Textbox(
                        label="è®­ç»ƒæ—¥å¿—", lines=20, interactive=False, autoscroll=True
                    )
                    train_progress = gr.Slider(
                        minimum=0, maximum=100, value=0, label="è®­ç»ƒè¿›åº¦ (%)"
                    )

                    gr.Markdown("""
                    **è¯´æ˜ï¼š**
                    - ç‚¹å‡»"å¼€å§‹è®­ç»ƒ"å¯åŠ¨è®­ç»ƒè¿‡ç¨‹
                    - è®­ç»ƒè¿‡ç¨‹ä¸­ä¼šå®æ—¶æ˜¾ç¤ºæŸå¤±å€¼å’Œè¿›åº¦
                    - å¯éšæ—¶ç‚¹å‡»"åœæ­¢è®­ç»ƒ"ä¸­æ–­ï¼ˆä¼šä¿å­˜å·²è®­ç»ƒçš„æƒé‡ï¼‰
                    - è®­ç»ƒç»“æœä¿å­˜åœ¨ `results/{å®éªŒåç§°}/` ç›®å½•
                    """)

            # ç»‘å®šæŒ‰é’®äº‹ä»¶
            start_train_btn.click(
                fn=run_training,
                inputs=[
                    train_dataset_path,
                    train_exp_name,
                    train_batch_size,
                    train_epochs,
                    train_lr,
                    train_save_interval,
                    train_use_gpu,
                ],
                outputs=train_output,
            )

            stop_train_btn.click(
                fn=stop_training,
                outputs=train_output,
            )

if __name__ == "__main__":
    print("Pre-loading models before launching UI...")
    load_models()
    print("Success: All models loaded. Launching UI...")
    print("=" * 50)
    print("Webç•Œé¢: http://127.0.0.1:7860")
    print("APIæ–‡æ¡£: http://127.0.0.1:7860/?view=api")
    print("=" * 50)
    demo.launch(server_name="0.0.0.0", server_port=7860, share=False)
